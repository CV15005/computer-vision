{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bizarre-slovak",
   "metadata": {},
   "source": [
    "# Цветовые модели, морфология, сегментация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc04c4-9e12-4c39-bffa-6bc2de2c4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-image\n",
    "# !pip install scikit-learn\n",
    "# !pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2hsv, label2rgb\n",
    "from skimage import data, color, io, img_as_float, feature\n",
    "from skimage.filters import threshold_otsu, try_all_threshold\n",
    "from skimage.util import img_as_ubyte, invert\n",
    "\n",
    "from skimage.transform import hough_circle, hough_circle_peaks, hough_ellipse\n",
    "from skimage.feature import canny, peak_local_max\n",
    "from skimage.draw import circle_perimeter, ellipse_perimeter\n",
    "from skimage.segmentation import clear_border, watershed\n",
    "\n",
    "from skimage.morphology import erosion, dilation, opening, closing, white_tophat, black_tophat, square\n",
    "from skimage.morphology import skeletonize, convex_hull_image, medial_axis, thin\n",
    "from skimage.morphology import disk\n",
    "from skimage.measure import label, regionprops\n",
    "\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from math import sqrt, exp\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d5634-15c8-4fb0-8e92-81e12cba468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from skimage import data, restoration, util, img_as_float, filters, color, morphology, segmentation\n",
    "from skimage.filters import threshold_otsu, try_all_threshold, threshold_niblack, threshold_sauvola, threshold_multiotsu\n",
    "\n",
    "from skimage.filters import sobel, rank\n",
    "from skimage.measure import label\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "from skimage.segmentation import watershed, mark_boundaries\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.transform import hough_line, hough_line_peaks, probabilistic_hough_line\n",
    " \n",
    "from skimage.feature import canny\n",
    "from skimage.draw import line\n",
    "from skimage.util import img_as_ubyte\n",
    "\n",
    "from skimage.data import binary_blobs\n",
    "\n",
    "from skimage.draw import ellipse\n",
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "from skimage.transform import rotate\n",
    "\n",
    "from skimage.filters import sobel\n",
    "from skimage.measure import label\n",
    "from skimage.util import img_as_float\n",
    "from skimage.feature import canny\n",
    "\n",
    "from skimage import data, segmentation, feature, future\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from functools import partial\n",
    "\n",
    "from matplotlib import cm\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from math import sqrt, exp\n",
    "import numpy as npshow_binary\n",
    "import scipy.ndimage as ndi\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-wallpaper",
   "metadata": {},
   "source": [
    "## 1. Цветовые пространства\n",
    "## 1.1. Переход из RGB в HSV\n",
    "Обычно объекты на изображениях имеют разные цвета (оттенки) и яркость, поэтому эти функции можно использовать для разделения разных областей изображения. В представлении RGB оттенок и яркость выражаются как линейная комбинация каналов R, G, B, тогда как в модели HSV они соответствуют отдельным каналам изображения (каналы Hue и Value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = data.coffee()\n",
    "hsv_img = rgb2hsv(rgb_img)\n",
    "hue_img = hsv_img[:, :, 0]\n",
    "value_img = hsv_img[:, :, 2]\n",
    "\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(8, 2))\n",
    "\n",
    "ax0.imshow(rgb_img)\n",
    "ax0.set_title(\"RGB image\")\n",
    "ax0.axis('off')\n",
    "ax1.imshow(hue_img, cmap='hsv')\n",
    "ax1.set_title(\"Hue channel\")\n",
    "ax1.axis('off')\n",
    "ax2.imshow(value_img,)\n",
    "ax2.set_title(\"Value channel\")\n",
    "ax2.axis('off')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-radar",
   "metadata": {},
   "source": [
    "На основании цветового пространства HSV можно проводить базовую сегментацию. Установим порог на канале Hue, чтобы отделить чашку от фона."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_threshold = 0.04\n",
    "binary_img = hue_img > hue_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(8, 3))\n",
    "\n",
    "ax0.hist(hue_img.ravel(), 512)\n",
    "ax0.set_title(\"Histogram of the Hue channel with threshold\")\n",
    "ax0.axvline(x=hue_threshold, color='r', linestyle='dashed', linewidth=2)\n",
    "ax0.set_xbound(0, 0.12)\n",
    "ax1.imshow(binary_img)\n",
    "ax1.set_title(\"Hue-thresholded image\")\n",
    "ax1.axis('off')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-messaging",
   "metadata": {},
   "source": [
    "Выполним дополнительную пороговую обработку для канала Value, чтобы частично удалить тень чашки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax0 = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "value_threshold = 0.10\n",
    "binary_img = (hue_img > hue_threshold) | (value_img < value_threshold)\n",
    "\n",
    "ax0.imshow(binary_img)\n",
    "ax0.set_title(\"Hue and value thresholded image\")\n",
    "ax0.axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-diagnosis",
   "metadata": {},
   "source": [
    "В OpenCV доступно более 150 методов преобразования цветового пространства. Но мы рассмотрим только два, которые наиболее широко используются: BGR ↔ Gray и BGR ↔ HSV.\n",
    "\n",
    "Для преобразования цвета мы используем функцию cv.cvtColor (input_image, flag), где flag определяет тип преобразования.\n",
    "\n",
    "Для преобразования BGR → Gray мы используем флаг cv.COLOR_BGR2GRAY. Аналогично для BGR → HSV мы используем флаг cv.COLOR_BGR2HSV. \n",
    "\n",
    "Рассмотрим существующие флаги:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = [i for i in dir(cv2) if i.startswith('COLOR_')]\n",
    "print(flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-marketplace",
   "metadata": {},
   "source": [
    "Следует отметить, для HSV диапазон оттенков составляет [0,179], диапазон насыщенности - [0,255], а диапазон значений - [0,255]. В разных программах используются разные масштабы. Поэтому, если вы сравниваете значения OpenCV с ними, необходимо нормализовать эти диапазоны."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-heaven",
   "metadata": {},
   "source": [
    "## 1.2. Отслеживание объекта на основе модели HSV\n",
    "Теперь, когда мы знаем, как преобразовать изображение BGR в HSV, мы можем использовать его для извлечения объекта из сцены. В HSV легче представить цвет, чем в цветовом пространстве BGR. Далее мы попытаемся извлечь объект синего цвета:\n",
    "- регистрируем каждый кадр видео\n",
    "- преобразовываем из BGR в HSV\n",
    "- устанавливаем порог изображения HSV для диапазона синего цвета\n",
    "- извлекаем только синий объект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# define range of blue color in HSV\n",
    "lower_color = np.array([100, 50, 50])\n",
    "upper_color = np.array([130, 255, 255])\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # Take each frame\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    # Convert BGR to HSV\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Threshold the HSV image to get only blue colors\n",
    "    mask = cv2.inRange(hsv, lower_color, upper_color)\n",
    "    \n",
    "    # Bitwise-AND mask and original image\n",
    "    res = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('mask', mask)\n",
    "    cv2.imshow('res', res)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-advice",
   "metadata": {},
   "source": [
    "## 2. Морфологическая обработка\n",
    "Морфологические преобразования - это группа простых операций, основанных на форме объектов на изображении. Обычно операция выполняется для двоичных изображений, иногда для серошкальных. Для реализации нужно два входа, один - это наше исходное изображение, второй - структурный элемент или ядро, которое определяет характер операции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectangular Kernel\n",
    "kern_rect = cv2.getStructuringElement(cv2.MORPH_RECT,(5,5))\n",
    "# Elliptical Kernel\n",
    "kern_ellip = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))\n",
    "# Cross-shaped Kernel\n",
    "kern_cross = cv2.getStructuringElement(cv2.MORPH_CROSS,(5,5))\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "ax = axes.ravel()\n",
    "ax[0] = plt.subplot(1, 3, 1, sharex=ax[0], sharey=ax[0])\n",
    "ax[1] = plt.subplot(1, 3, 2, sharex=ax[0], sharey=ax[0])\n",
    "ax[2] = plt.subplot(1, 3, 3, sharex=ax[0], sharey=ax[0])\n",
    "\n",
    "ax[0].imshow(kern_rect, cmap=plt.cm.gray)\n",
    "ax[0].set_title('Rectangular Kernel')\n",
    "\n",
    "ax[1].imshow(kern_ellip, cmap=plt.cm.gray)\n",
    "ax[1].set_title('Elliptical Kernel')\n",
    "\n",
    "ax[2].imshow(kern_cross, cmap=plt.cm.gray)\n",
    "ax[2].set_title('Cross-shaped Kernel')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-museum",
   "metadata": {},
   "source": [
    "Считаем изображение и определим функцию для демонстрации эффектов морфологической обработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_phantom = img_as_ubyte(data.shepp_logan_phantom())\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(orig_phantom, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(original, filtered, filter_name):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 6), sharex=True,\n",
    "                                   sharey=True)\n",
    "    ax1.imshow(original, cmap=plt.cm.gray)\n",
    "    ax1.set_title('original')\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(filtered, cmap=plt.cm.gray)\n",
    "    ax2.set_title(filter_name)\n",
    "    ax2.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-taiwan",
   "metadata": {},
   "source": [
    "## 2.1. Эрозия\n",
    "Морфологическая эрозия устанавливает пиксель в (i, j) на минимум по всем пикселям в окрестности с центром в (i, j). Элемент структурирования, selem, переданный в erosion, представляет собой логический массив, который описывает эту окрестность. Ниже мы используем диск для создания кругового структурирующего элемента, который используется в большинстве следующих примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "selem = disk(6)\n",
    "eroded = erosion(orig_phantom, selem)\n",
    "plot_comparison(orig_phantom, eroded, 'erosion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-logistics",
   "metadata": {},
   "source": [
    "## 2.2. Дилатация\n",
    "Морфологическая дилатация устанавливает пиксель в (i, j) на максимум по всем пикселям в окрестности с центром в (i, j). Дилатация увеличивает яркие области и уменьшает темные области"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilated = dilation(orig_phantom, selem)\n",
    "plot_comparison(orig_phantom, dilated, 'dilation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-combining",
   "metadata": {},
   "source": [
    "## 2.3. Размыкание\n",
    "Морфологическое размыкание на изображении определяется как эрозия с последующей дилатацией. Размыкание может удалить небольшие яркие пятна и соединить небольшие темные разрывы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "opened = opening(orig_phantom, selem)\n",
    "plot_comparison(orig_phantom, opened, 'opening')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-helena",
   "metadata": {},
   "source": [
    "Пример применения размыкания с помощью OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_j = cv2.imread('j_opening.png', 0)\n",
    "_, im_j = cv2.threshold(im_j, 127, 255, cv2.THRESH_BINARY)\n",
    "kernel = np.ones((7,7), np.uint8)\n",
    "\n",
    "opening_j = cv2.morphologyEx(im_j, cv2.MORPH_OPEN, kernel)\n",
    "plot_comparison(im_j, opening_j, 'opening opencv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-sodium",
   "metadata": {},
   "source": [
    "## 2.4. Замыкание\n",
    "Морфологическое замыкание изображения определяется как дилатация, за которой следует эрозия. Замыкание позволяет удалить небольшие темные пятна и соединить небольшие светлые разрывы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "phantom = orig_phantom.copy()\n",
    "phantom[10:30, 200:210] = 0\n",
    "\n",
    "closed = closing(phantom, selem)\n",
    "plot_comparison(phantom, closed, 'closing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-wrestling",
   "metadata": {},
   "source": [
    "Пример применения OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_j = cv2.imread('j_closing.png',0)\n",
    "_, im_j = cv2.threshold(im_j, 127, 255, cv2.THRESH_BINARY)\n",
    "kernel = np.ones((7,7), np.uint8)\n",
    "\n",
    "closing_j = cv2.morphologyEx(im_j, cv2.MORPH_CLOSE, kernel)\n",
    "plot_comparison(im_j, closing_j, 'closing opencv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-bridge",
   "metadata": {},
   "source": [
    "## 2.5. Морфологический градиент\n",
    "Морфологический градиент вычисляется как разница между дилатацией и эрозией изображения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = cv2.morphologyEx(closing_j, cv2.MORPH_GRADIENT, kernel)\n",
    "\n",
    "plot_comparison(closing_j, gradient, 'closing opencv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-roulette",
   "metadata": {},
   "source": [
    "## 2.6. Tophat-фильтрация\n",
    "White_tophat преобразования изображения определяется как изображение за вычетом его морфологического размыкания. Эта операция возвращает яркие точки изображения, которые меньше структурного элемента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "phantom = orig_phantom.copy()\n",
    "phantom[340:350, 200:210] = 255\n",
    "phantom[100:110, 200:210] = 0\n",
    "\n",
    "w_tophat = white_tophat(phantom, selem)\n",
    "plot_comparison(phantom, w_tophat, 'white tophat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-merit",
   "metadata": {},
   "source": [
    "С помощью tophat преобразования можно выполнять фильтрацию. Например, можно выполнять удаление мелких объектов на изображениях в градациях серого. В данном случае воспользуемся преобразование white tophat, которое определяется как разница между входным изображением и его размыканием."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = color.rgb2gray(data.hubble_deep_field())[:500, :500]\n",
    "\n",
    "selem_th =  disk(2)\n",
    "res = white_tophat(image, selem_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(20, 8))\n",
    "ax[0].set_title('Original')\n",
    "ax[0].imshow(image, cmap='gray')\n",
    "ax[1].set_title('White tophat')\n",
    "ax[1].imshow(res, cmap='gray')\n",
    "ax[2].set_title('Complementary')\n",
    "ax[2].imshow(image - res, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-european",
   "metadata": {},
   "source": [
    "Применение Black Tophat. Black_tophat изображения определяется как его морфологическое замыкание за вычетом исходного изображения. Эта операция возвращает темные пятна изображения, которые меньше структурного элемента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tophat = black_tophat(phantom, selem)\n",
    "plot_comparison(phantom, b_tophat, 'black tophat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-rental",
   "metadata": {},
   "source": [
    "## 2.7. Формирование остова\n",
    "Формирование остова используется для уменьшения каждого связного компонента в двоичном изображении до скелета объекта шириной в один пиксель. Важно отметить, что эта операция выполняется только для двоичных изображений.  \n",
    "\n",
    "Формирование скелета объекта может быть полезно для извлечения признаков и / или представления топологии объекта.\n",
    "\n",
    "Скелетонизация работает на основе последовательных проходов изображения. На каждом проходе граничные пиксели идентифицируются и удаляются при условии, что они не нарушают связность соответствующего объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "horse = data.horse()\n",
    "\n",
    "sk = skeletonize(horse == 0)\n",
    "plot_comparison(horse, sk, 'skeletonize')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-bennett",
   "metadata": {},
   "source": [
    "__Подходы к построению остова__\n",
    "Метод skeletonize работает, выполняя последовательные проходы изображения, удаляя пиксели на границах объекта. Это продолжается до тех пор, пока больше не удастся удалить пиксели. Изображение коррелируется с маской, которая присваивает каждому пикселю номер в диапазоне [0… 255], соответствующий каждому возможному шаблону из его 8 соседних пикселей. Затем используется таблица поиска для присвоения пикселям значения 0, 1, 2 или 3, которые выборочно удаляются во время итераций.\n",
    "\n",
    "Существует метод построения остова по методу Lee (skeletonize (..., method = 'lee')), который использует структуру данных октодерева для исследования окрестности пикселя размером 3x3х3. Алгоритм выполняется путем итеративного прохождения по изображению и удаления пикселей на каждой итерации, пока изображение не перестанет изменяться. Каждая итерация состоит из двух шагов: во-первых, составляется список кандидатов на удаление; затем пиксели из этого списка последовательно перепроверяются, чтобы лучше сохранить связность изображения.\n",
    "\n",
    "Обратите внимание, что метод Ли разработан для использования с трехмерными изображениями и выбирается для них автоматически. В иллюстративных целях мы применяем этот алгоритм к двухмерному изображению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = data.binary_blobs(200, blob_size_fraction=.2,\n",
    "                          volume_fraction=.35, rng=1)\n",
    "\n",
    "skeleton = skeletonize(blobs)\n",
    "skeleton_lee = skeletonize(blobs, method='lee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(blobs, cmap=plt.cm.gray)\n",
    "ax[0].set_title('original')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(skeleton, cmap=plt.cm.gray)\n",
    "ax[1].set_title('skeletonize')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(skeleton_lee, cmap=plt.cm.gray)\n",
    "ax[2].set_title('skeletonize (Lee 94)')\n",
    "ax[2].axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-sullivan",
   "metadata": {},
   "source": [
    "Построение средней оси объекта - это получение совокупности всех точек, имеющих более одной ближайшей точки на границе объекта. Среднюю ось объекта часто называют топологическим каркасом, потому что это каркас объекта шириной 1 пиксель с той же связностью, что и исходный объект.\n",
    "\n",
    "Здесь мы используем преобразование средней оси для вычисления ширины объектов переднего плана. Поскольку функция medial_axis возвращает преобразование расстояния в дополнение к средней оси (с аргументом return_distance = True), с помощью этой функции можно вычислить расстояние до фона для всех точек медиальной оси. Это дает оценку локальной ширины объектов.\n",
    "\n",
    "Для объекта с меньшим количеством ветвей предпочтительнее использовать построение остова с помощью метода skeletonize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "blobs = data.binary_blobs(200, blob_size_fraction=.2,\n",
    "                          volume_fraction=.35, rng=1)\n",
    "\n",
    "# Compute the medial axis (skeleton) and the distance transform\n",
    "skel, distance = medial_axis(blobs, return_distance=True)\n",
    "\n",
    "# Compare with other skeletonization algorithms\n",
    "skeleton = skeletonize(blobs)\n",
    "skeleton_lee = skeletonize(blobs, method='lee')\n",
    "\n",
    "# Distance to the background for pixels of the skeleton\n",
    "dist_on_skel = distance * skel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(blobs, cmap=plt.cm.gray)\n",
    "ax[0].set_title('original')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(dist_on_skel, cmap='magma')\n",
    "ax[1].contour(blobs, [0.5], colors='w')\n",
    "ax[1].set_title('medial_axis')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(skeleton, cmap=plt.cm.gray)\n",
    "ax[2].set_title('skeletonize')\n",
    "ax[2].axis('off')\n",
    "\n",
    "ax[3].imshow(skeleton_lee, cmap=plt.cm.gray)\n",
    "ax[3].set_title(\"skeletonize (Lee 94)\")\n",
    "ax[3].axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-field",
   "metadata": {},
   "source": [
    "## 2.8. Морфологическое утончение\n",
    "Морфологическое утончение работает по тому же принципу, что и скелетонизация: выполняется удаление пикселей с границ на каждой итерации, пока ни один из них не может быть удален без изменения связности. Различные правила удаления могут ускорить утончение и привести к получению разных окончательных остовов.\n",
    "\n",
    "Функция морфологического утончения принимает необязательный аргумент max_num_iter, чтобы ограничить количество итераций прореживания и, таким образом, создать относительно более толстый каркас."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = invert(data.horse())\n",
    "skeleton = skeletonize(image)\n",
    "thinned = thin(image)\n",
    "thinned_partial = thin(image, max_num_iter=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(image, cmap=plt.cm.gray)\n",
    "ax[0].set_title('original')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(skeleton, cmap=plt.cm.gray)\n",
    "ax[1].set_title('skeleton')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(thinned, cmap=plt.cm.gray)\n",
    "ax[2].set_title('thinned')\n",
    "ax[2].axis('off')\n",
    "\n",
    "ax[3].imshow(thinned_partial, cmap=plt.cm.gray)\n",
    "ax[3].set_title('partially thinned')\n",
    "ax[3].axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-representative",
   "metadata": {},
   "source": [
    "## 2.9. Формирование выпуклой оболочки\n",
    "Выпуклая оболочка изображения - это набор пикселей, включенных в наименьший выпуклый многоугольник, который окружает все белые пиксели входного изображения. Обратите внимание, что эта операция также выполняется с бинарными изображениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "horse = data.horse()\n",
    "hull1 = convex_hull_image(horse == 0)\n",
    "plot_comparison(horse, hull1, 'convex hull')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-queue",
   "metadata": {},
   "source": [
    "Как показано на рисунке, convx_hull_image дает самый маленький многоугольник, который полностью покрывает белый цвет или значение True на изображении.\n",
    "\n",
    "Если мы добавим к изображению небольшую зернистость, мы увидим, как выпуклая оболочка адаптируется, чтобы охватить это зерно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_mask = horse == 0\n",
    "horse_mask[45:50, 75:80] = 1\n",
    "\n",
    "hull2 = convex_hull_image(horse_mask)\n",
    "plot_comparison(horse_mask, hull2, 'convex hull')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "hull_diff = img_as_float(hull2.copy())\n",
    "hull_diff[horse_mask] = 2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(hull_diff, cmap=plt.cm.gray)\n",
    "ax.set_title('Difference')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e67132-c5ef-4e48-855d-ab599ee74817",
   "metadata": {},
   "source": [
    "## 3. Сегментация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421227d7-d31e-48f7-88e6-e207dd604dbf",
   "metadata": {},
   "source": [
    "## 3.1. Пороговая сегментация\n",
    "## 3.1.1. Базовая сегментация на основе бинаризации по глобальному яркостному порогу\n",
    "__Метод Оцу__ вычисляет «оптимальный» порог (отмечен красной линией на гистограмме ниже) путем максимизации дисперсии между двумя классами пикселей, которые разделены пороговым значением. Равным образом этот порог минимизирует внутриклассовую дисперсию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f2dc8-1c40-4cf1-adca-bf9528a5a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_binary(image, binary):\n",
    "    fig, axes = plt.subplots(ncols=3, figsize=(16, 4))\n",
    "    ax = axes.ravel()\n",
    "    ax[0] = plt.subplot(1, 3, 1)\n",
    "    ax[1] = plt.subplot(1, 3, 2)\n",
    "    ax[2] = plt.subplot(1, 3, 3, sharex=ax[0], sharey=ax[0])\n",
    "\n",
    "    ax[0].imshow(image, cmap=plt.cm.gray)\n",
    "    ax[0].set_title('Original')\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    ax[1].hist(image.ravel(), bins=256)\n",
    "    ax[1].set_title('Histogram')\n",
    "    ax[1].axvline(thresh, color='r')\n",
    "\n",
    "    ax[2].imshow(binary, cmap=plt.cm.gray)\n",
    "    ax[2].set_title('Thresholded')\n",
    "    ax[2].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899818d8-4957-4307-a843-da5aaedd5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data.coins()\n",
    "thresh = threshold_otsu(image)\n",
    "binary = image > thresh\n",
    "\n",
    "show_binary(image, binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae956378-b517-4713-9e68-b0e5dfd2f82b",
   "metadata": {},
   "source": [
    "Негативное влияние неравномерной освещенности на результат сегментации по порогу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b85926-266b-47b0-bbc4-2e1a423554b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = try_all_threshold(image, figsize=(10, 8), verbose=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eb27e6-f9c3-495d-a7a3-f9a98dd6d8c6",
   "metadata": {},
   "source": [
    "## 3.1.2. Устранение неравномерности освещения\n",
    "__Алгоритм катящегося мяча для оценки интенсивности фона__.\n",
    "Алгоритм катящегося мяча оценивает интенсивность фона изображения в оттенках серого в случае неравномерной экспозиции. Он часто используется в биомедицинской обработке изображений и был впервые предложен Стэнли Р. Стернбергом в 1983 году. \n",
    "\n",
    "Алгоритм работает как фильтр и интуитивно понятен. Мы представляем изображение как поверхность, на которой блоки единичного размера наложены друг на друга вместо каждого пикселя. Количество блоков и, следовательно, высота поверхности определяется интенсивностью пикселя. Чтобы получить интенсивность фона в желаемом (пиксельном) положении, мы представляем погружение шара под поверхность в желаемом месте. Когда мяч полностью покрыт блоками, вершина шара определяет интенсивность фона в этой позиции. Затем мы можем катать этот шар под поверхностью, чтобы получить значения фона для всего изображения.\n",
    "\n",
    "Scikit-image реализует обобщенную версию этого алгоритма, который позволяет вам использовать произвольные формы в качестве ядра и работает с n-мерными изображениями. Это позволяет напрямую фильтровать изображения RGB или фильтровать стеки изображений по любому (или всем) пространственным измерениям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534632df-e422-448d-a60e-fa114c583de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "background = restoration.rolling_ball(image)\n",
    "image_result = image - background\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "\n",
    "ax[0].imshow(image, cmap='gray')\n",
    "ax[0].set_title('Original image')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(background, cmap='gray')\n",
    "ax[1].set_title('Background')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(image_result, cmap='gray')\n",
    "ax[2].set_title('Result')\n",
    "ax[2].axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49b88d-a349-40e6-a473-7a174461b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = threshold_otsu(image_result)\n",
    "binary = image_result > thresh\n",
    "\n",
    "show_binary(image_result, binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c67d26-bb9f-436a-a17e-4476a39f2dc3",
   "metadata": {},
   "source": [
    "## 3.1.3. Локальные методы бинаризации\n",
    "Для учета эффектов неравномерности освещенности предлагается применять специализированные методы бинаризации. __Пороги Ниблака и Саувола__ - это локальные методы определения пороговых значений, которые полезны для изображений с неоднородным фоном, особенно при распознавании текста. Вместо расчета единого глобального порога для всего изображения, несколько пороговых значений вычисляются для каждого пикселя с использованием определенных формул, которые учитывают среднее значение и стандартное отклонение локальной окрестности (определяемой окном с центром вокруг пикселя)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79949180-d118-4ff5-bd44-15a7c94f34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data.page()\n",
    "binary_global = image > threshold_otsu(image)\n",
    "\n",
    "window_size = 25\n",
    "thresh_niblack = threshold_niblack(image, window_size=window_size, k=0.8)\n",
    "thresh_sauvola = threshold_sauvola(image, window_size=window_size)\n",
    "\n",
    "binary_niblack = image > thresh_niblack\n",
    "binary_sauvola = image > thresh_sauvola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad94b58-c352-412b-92d7-2c747f0aae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(image, cmap=plt.cm.gray)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Global Threshold')\n",
    "plt.imshow(binary_global, cmap=plt.cm.gray)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(binary_niblack, cmap=plt.cm.gray)\n",
    "plt.title('Niblack Threshold')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(binary_sauvola, cmap=plt.cm.gray)\n",
    "plt.title('Sauvola Threshold')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e189e-7b86-4ac0-bb2b-b179e35bd93b",
   "metadata": {},
   "source": [
    "## 3.1.4. Обработка с несколькими порогами\n",
    "Множественные пороги Оцу - это алгоритм пороговой обработки, который используется для разделения пикселей входного изображения на несколько различных классов, каждый из которых получается в соответствии с интенсивностью уровней серого в изображении.\n",
    "\n",
    "Multi-Otsu рассчитывает несколько пороговых значений, определяемых количеством желаемых классов. По умолчанию количество классов равно 3: для получения трех классов алгоритм возвращает два пороговых значения. Они представлены красными линиями на гистограмме ниже. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d86d10-fea3-462f-b542-56e50a275458",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data.camera()\n",
    "\n",
    "# Applying multi-Otsu threshold for the default value, generating\n",
    "# three classes.\n",
    "thresholds = threshold_multiotsu(image)\n",
    "\n",
    "# Using the threshold values, we generate the three regions.\n",
    "regions = np.digitize(image, bins=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9928ac-8b6e-48ca-976e-062dd75f88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "# Plotting the original image.\n",
    "ax[0].imshow(image, cmap='gray')\n",
    "ax[0].set_title('Original')\n",
    "ax[0].axis('off')\n",
    "\n",
    "# Plotting the histogram and the two thresholds obtained from\n",
    "# multi-Otsu.\n",
    "ax[1].hist(image.ravel(), bins=255)\n",
    "ax[1].set_title('Histogram')\n",
    "for thresh in thresholds:\n",
    "    ax[1].axvline(thresh, color='r')\n",
    "\n",
    "# Plotting the Multi Otsu result.\n",
    "ax[2].imshow(regions, cmap='jet')\n",
    "ax[2].set_title('Multi-Otsu result')\n",
    "ax[2].axis('off')\n",
    "\n",
    "plt.subplots_adjust()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ee935-68f3-4545-a4f3-2b1ec17cf507",
   "metadata": {},
   "outputs": [],
   "source": [
    "man_mask = np.int8(regions == 0)\n",
    "plt.imshow(man_mask, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70c3c2f-2d35-4c7b-aaab-845627dfedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "selem = disk(6)\n",
    "opened = opening(man_mask, selem)\n",
    "\n",
    "plt.imshow(opened, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b19f6-b9e2-45fd-8601-7ef76dcf9f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opened_mask = opened == 1\n",
    "hull2 = convex_hull_image(opened_mask)\n",
    "hull_diff = img_as_float(hull2.copy())\n",
    "hull_diff[opened] = 1\n",
    "resulting = hull_diff * image\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(resulting, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-emerald",
   "metadata": {},
   "source": [
    "## 3.2. Детектор границ Canny\n",
    "Фильтр Кэнни - это многоступенчатый алгоритм детектирования границ. Данный фильтр основан на производной Гаусса для оценки интенсивности градиентов. Гауссовый фильтр уменьшает эффект шума, присутствующего в изображении. Затем потенциальные границы прореживаются до примитивов толщиной в 1 пиксель путем удаления немаксимумов градиента. Наконец, краевые пиксели сохраняются или удаляются с использованием порогового значения гистерезиса.\n",
    "\n",
    "Canny имеет три настраиваемых параметра: ширину гауссиана (чем шумнее изображение, тем больше ширина), нижний и верхний пороги для порогового значения гистерезиса.\n",
    "\n",
    "1. Поскольку обнаружение краев чувствительно к шуму в изображении, первым этапом является удаление шума в изображении с помощью гауссовского фильтра 5x5.  \n",
    "  \n",
    "2. Нахождение градиента интенсивности изображения. Сглаженное изображение фильтруется с помощью ядра Собеля как в горизонтальном, так и в вертикальном направлении, чтобы получить первую производную в горизонтальном направлении (Gx) и вертикальном направлении (Gy). Из этих двух изображений мы можем найти градиент и направление границ для каждого пикселя:\n",
    "$$ Edge\\_Gradient \\; (G) = \\sqrt{G_x^2 + G_y^2} $$\n",
    "\n",
    "$$ Angle \\; (\\theta) = \\tan^{-1} \\bigg(\\frac{G_y}{G_x}\\bigg)$$\n",
    "3. Подавление немаксимумов. После получения величины и направления градиента выполняется полное сканирование изображения для удаления любых нежелательных пикселей, которые могут не принадлежать границам. Для этого проверяется пиксель: является ли он локальным максимумом в окрестности в направлении градиента.\n",
    "<img src=\"https://docs.opencv.org/master/nms.jpg\" width=\"300\" height=\"300\">  \n",
    "Точка А находится на краю (в вертикальном направлении). Направление градиента перпендикулярно краю. Точки B и C находятся в направлениях градиента. Таким образом, точка A сравнивается с точками B и C, чтобы оценить, образует ли она локальный максимум. В противном случае, значение подавляется (обнуляется). В результате, мы получаем двоичное изображение с «тонкими границами».\n",
    "  \n",
    "4. Оценка порогового значения гистерезиса. На этом этапе решается, какие из границ действительно являются границами, а какие нет. Для этого нам нужны два пороговых значения: minVal и maxVal. Любые края с градиентом интенсивности больше maxVal обязательно будут краями, а те, что ниже minVal, обязательно не будут краями. Те границы, что находятся между этими двумя порогами, классифицируются как границы в зависимости от их связности. Если они соединены с пикселями с четкими границами, они считаются частью границ. В противном случае они также отбрасываются.\n",
    "<img src=\"https://docs.opencv.org/master/hysteresis.jpg\" width=\"400\" height=\"400\"> \n",
    "В указанном примере граница A выше maxVal, поэтому считается \"надежной границей\". Хотя край C ниже maxVal, он соединен с ребром A, поэтому он также считается допустимым, и мы получаем полную кривую. Но ребро B, хотя оно выше minVal и находится в той же области, что и ребро C, не связано с каким-либо \"точным краем\", поэтому оно отбрасывается. На этом этапе также удаляются мелкие шумы.  \n",
    "\n",
    "__Пример использования детектора границ Canny в библиотеке OpenCV__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('messi.jpg', 0)\n",
    "edges = cv2.Canny(img, 100, 200)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(20, 8))\n",
    "plt.subplot(121),plt.imshow(img,cmap = 'gray')\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(122),plt.imshow(edges,cmap = 'gray')\n",
    "plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-hours",
   "metadata": {},
   "source": [
    "__Пример использования детектора границ Canny в библиотеке Scikit-image__  \n",
    "Зададим размытое изображение с шумом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = np.zeros((128, 128))\n",
    "im[32:-32, 32:-32] = 1\n",
    "\n",
    "im = ndi.rotate(im, 15, mode='constant')\n",
    "im = ndi.gaussian_filter(im, 4)\n",
    "im += 0.2 * np.random.random(im.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-metadata",
   "metadata": {},
   "source": [
    "Применим детектор границ Canny с различными значениями sigma промежуточного Гауссова фильтра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges1 = feature.canny(im)\n",
    "edges2 = feature.canny(im, sigma=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(8, 3),\n",
    "                                    sharex=True, sharey=True)\n",
    "\n",
    "ax1.imshow(im, cmap=plt.cm.gray)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('noisy image', fontsize=20)\n",
    "\n",
    "ax2.imshow(edges1, cmap=plt.cm.gray)\n",
    "ax2.axis('off')\n",
    "ax2.set_title(r'Canny filter, $\\sigma=1$', fontsize=20)\n",
    "\n",
    "ax3.imshow(edges2, cmap=plt.cm.gray)\n",
    "ax3.axis('off')\n",
    "ax3.set_title(r'Canny filter, $\\sigma=3$', fontsize=20)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff116ef-a916-4c41-811e-0cf28a80d8d9",
   "metadata": {},
   "source": [
    "## 3.3. Сегментирование на основе контуров и границ\n",
    "## 3.3.1. Поиск прямых линий на основе преобразования Хафа\n",
    "Преобразование Хафа в его простейшей форме - это метод обнаружения прямых.\n",
    "\n",
    "В следующем примере мы создаем изображение с пересечением линии. Затем мы используем преобразование Хафа, чтобы исследовать пространство параметров для прямых линий, которые могут проходить через изображение.\n",
    "\n",
    "Обычно линии параметризуются как y = mx + c с градиентом m и точкой пересечения y c. Однако это означало бы, что m стремится к бесконечности для вертикальных линий. Вместо этого мы строим отрезок, перпендикулярный линии, ведущий в начало координат. Линия представлена длиной этого сегмента r и углом, который он составляет с осью x, θ.\n",
    "\n",
    "Преобразование Хафа создает массив гистограмм, представляющий пространство параметров (то есть матрицу M × N, для M различных значений радиуса и N различных значений θ). Для каждой комбинации параметров, r и θ, мы затем находим количество ненулевых пикселей во входном изображении, которые будут располагаться близко к соответствующей строке, и соответствующим образом увеличиваем массив в позиции (r, θ).\n",
    "\n",
    "Локальные максимумы на полученной гистограмме указывают параметры наиболее вероятных линий. В нашем примере максимумы наблюдаются под углом 45 и 135 градусов, что соответствует углам вектора нормали каждой линии.\n",
    "\n",
    "Другой подход - это прогрессивное вероятностное преобразование Хафа. Такой подход основан на предположении, что использование случайного подмножества точек голосования дает хорошее приближение к фактическому результату, и что линии могут быть извлечены во время процесса голосования путем обхода связанных компонентов. Это возвращает начало и конец каждого сегмента линии.\n",
    "\n",
    "Функция probabilistic_hough имеет три параметра: общий порог, который применяется к аккумулятору Хафа, минимальная длина строки и промежуток между строками, который влияет на объединение строк. В приведенном ниже примере мы находим строки длиной более 10 пикселей с зазором менее 3 пикселей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b31fc-f952-4d49-8b00-2451ef360e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing test image\n",
    "image = np.zeros((200, 200))\n",
    "idx = np.arange(25, 175)\n",
    "image[idx, idx] = 255\n",
    "image[line(45, 25, 25, 175)] = 255\n",
    "image[line(25, 135, 175, 155)] = 255\n",
    "\n",
    "# Classic straight-line Hough transform\n",
    "# Set a precision of 0.5 degree.\n",
    "tested_angles = np.linspace(-np.pi / 2, np.pi / 2, 360, endpoint=False)\n",
    "h, theta, d = hough_line(image, theta=tested_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd66a6c6-0403-47d8-aece-640d74357bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating figure 1\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(image, cmap=cm.gray)\n",
    "ax[0].set_title('Input image')\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "angle_step = 0.5 * np.diff(theta).mean()\n",
    "d_step = 0.5 * np.diff(d).mean()\n",
    "bounds = [np.rad2deg(theta[0] - angle_step),\n",
    "          np.rad2deg(theta[-1] + angle_step),\n",
    "          d[-1] + d_step, d[0] - d_step]\n",
    "ax[1].imshow(np.log(1 + h), extent=bounds, cmap=cm.gray, aspect=1 / 1.5)\n",
    "ax[1].set_title('Hough transform')\n",
    "ax[1].set_xlabel('Angles (degrees)')\n",
    "ax[1].set_ylabel('Distance (pixels)')\n",
    "ax[1].axis('image')\n",
    "\n",
    "ax[2].imshow(image, cmap=cm.gray)\n",
    "ax[2].set_ylim((image.shape[0], 0))\n",
    "ax[2].set_axis_off()\n",
    "ax[2].set_title('Detected lines')\n",
    "\n",
    "for _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n",
    "    (x0, y0) = dist * np.array([np.cos(angle), np.sin(angle)])\n",
    "    ax[2].axline((x0, y0), slope=np.tan(angle + np.pi/2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-cross",
   "metadata": {},
   "source": [
    "## 3.3.2. Круговое преобразование Хафа\n",
    "Преобразование Хафа в его простейшей форме - это метод обнаружения прямых линий, но его также можно использовать для обнаружения кругов или эллипсов. Алгоритм предполагает, что границы на изображении уже обнаружены, а шумы или отсутствующие точки/разрыва устранены.\n",
    "\n",
    "### Алгоритм поиска круговых объектов\n",
    "Допустим на белом фоне располагается черный круг. Для начала мы задаем диапазон радиусов, по которым могут быть построен потенциальный круг. Этот круг наносится на каждый черный пиксель исходного изображения, и координаты этого круга учитываются в аккумуляторе. Далее подбирается положение центра круга, которое получает наивысший балл в аккумуляторе.  \n",
    "Обратите внимание, что размер аккумуляторного объекта больше, чем у исходного изображения, чтобы определять центры кругов за пределами кадра.\n",
    "\n",
    "### Пример обнаружения кругов с помощью Scikit-image\n",
    "В следующем примере преобразование Хафа используется для определения положения монет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load picture and detect edges\n",
    "image = img_as_ubyte(data.coins()[160:230, 70:270])\n",
    "edges = canny(image, sigma=3, low_threshold=10, high_threshold=50)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(20, 8))\n",
    "plt.subplot(121),plt.imshow(image,cmap = 'gray')\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(122),plt.imshow(edges,cmap = 'gray')\n",
    "plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect two radii\n",
    "hough_radii = np.arange(20, 35, 2)\n",
    "hough_res = hough_circle(edges, hough_radii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(hough_res[1],cmap = 'gray')\n",
    "plt.title('Hough Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the most prominent 3 circles\n",
    "accums, cx, cy, radii = hough_circle_peaks(hough_res, hough_radii,\n",
    "                                           total_num_peaks=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw them\n",
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(10, 4))\n",
    "image = color.gray2rgb(image)\n",
    "for center_y, center_x, radius in zip(cy, cx, radii):\n",
    "    circy, circx = circle_perimeter(center_y, center_x, radius,\n",
    "                                    shape=image.shape)\n",
    "    image[circy, circx] = (220, 20, 20)\n",
    "\n",
    "ax.imshow(image, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-listing",
   "metadata": {},
   "source": [
    "Получены координаты центров и радиусов объектов на изображении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "cx, cy, radii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-carbon",
   "metadata": {},
   "source": [
    "### Алгоритм поиска эллипсов\n",
    "Алгоритм берет две разные точки, принадлежащие эллипсу. Предполагается, что это главная ось. Цикл по всем другим точкам определяет, насколько эллипс включает их. Хорошее совпадение соответствует высоким значениям аккумулятора. По пиковым значениям аккумулятора выбирается значение, определяющее локализацию эллипса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load picture, convert to grayscale and detect edges\n",
    "image_rgb = data.coffee()[0:220, 160:420]\n",
    "image_gray = color.rgb2gray(image_rgb)\n",
    "edges = canny(image_gray, sigma=2.0,\n",
    "              low_threshold=0.55, high_threshold=0.8)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "plt.subplot(121),plt.imshow(image_gray,cmap = 'gray')\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(122),plt.imshow(edges,cmap = 'gray')\n",
    "plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a Hough Transform\n",
    "# The accuracy corresponds to the bin size of a major axis.\n",
    "# The value is chosen in order to get a single high accumulator.\n",
    "# The threshold eliminates low accumulators\n",
    "result = hough_ellipse(edges, accuracy=20, threshold=250,\n",
    "                       min_size=100, max_size=120)\n",
    "result.sort(order='accumulator')\n",
    "\n",
    "# Estimated parameters for the ellipse\n",
    "best = list(result[-1])\n",
    "yc, xc, a, b = [int(round(x)) for x in best[1:5]]\n",
    "orientation = best[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the ellipse on the original image\n",
    "cy, cx = ellipse_perimeter(yc, xc, a, b, orientation)\n",
    "image_rgb[cy, cx] = (0, 0, 255)\n",
    "# Draw the edge (white) and the resulting ellipse (red)\n",
    "edges = color.gray2rgb(img_as_ubyte(edges))\n",
    "edges[cy, cx] = (250, 0, 0)\n",
    "\n",
    "fig2, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(10, 4),\n",
    "                                sharex=True, sharey=True)\n",
    "\n",
    "ax1.set_title('Original picture')\n",
    "ax1.imshow(image_rgb)\n",
    "\n",
    "ax2.set_title('Edge (white) and result (red)')\n",
    "ax2.imshow(edges)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-stephen",
   "metadata": {},
   "source": [
    "### Применение библиотеки OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img_as_ubyte(data.coins()[160:230, 70:270])\n",
    "cimg = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "circles = cv2.HoughCircles(img, cv2.HOUGH_GRADIENT, 1, 30,\n",
    "                            param1=65, param2=40, minRadius=10, maxRadius=35)\n",
    "circles = np.uint16(np.around(circles))\n",
    "\n",
    "for i in circles[0, :]:\n",
    "    # draw the outer circle\n",
    "    cv2.circle(cimg, (i[0],i[1]), i[2], (0,255,0), 2)\n",
    "    # draw the center of the circle\n",
    "    cv2.circle(cimg, (i[0],i[1]), 2, (0,0,255), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cimg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-sapphire",
   "metadata": {},
   "source": [
    "## 3.4. Сегментация на основе водораздела\n",
    "Сегментация на основе водораздела - это классический алгоритм, используемый для сегментации, то есть для разделения различных объектов на изображении.\n",
    "\n",
    "Любое изображение в оттенках серого можно рассматривать как топографическую поверхность, где высокая интенсивность обозначает пики и холмы, а низкая интенсивность обозначает впадины.\n",
    "\n",
    "Начиная с определяемых пользователем маркеров, алгоритм водораздела обрабатывает значения пикселей как местную топографию (высоту). Алгоритм затопляет бассейны от маркеров до тех пор, пока бассейны, относящиеся к разным маркерам (уровням интенсивности), не встретятся на линиях водоразделов. Во многих случаях маркеры выбираются в качестве локальных минимумов изображения, из которых затопляются бассейны.\n",
    "\n",
    "### Процесс сегментации, реализуемой в библиотеке Scikit-image. \n",
    "В приведенном ниже примере необходимо разделить два перекрывающихся круга. Для этого вычисляется изображение, представляющее собой расстояние до фона. Максимумы этого расстояния выбраны в качестве маркеров, и затопление бассейнов от таких маркеров разделяет два круга вдоль линии водораздела. Подобный подход позволяет проводить сегментацию перекрывающихся объектов.\n",
    "\n",
    "Сгенерируем два перекрывающихся круга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.indices((80, 80))\n",
    "x1, y1, x2, y2 = 28, 28, 44, 52\n",
    "r1, r2 = 16, 20\n",
    "mask_circle1 = (x - x1)**2 + (y - y1)**2 < r1**2\n",
    "mask_circle2 = (x - x2)**2 + (y - y2)**2 < r2**2\n",
    "image = np.logical_or(mask_circle1, mask_circle2)\n",
    "\n",
    "plt.imshow(image, cmap=plt.cm.gray)\n",
    "plt.title('Overlapping objects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-woman",
   "metadata": {},
   "source": [
    "Определим центры объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = ndi.distance_transform_edt(image)\n",
    "coords = peak_local_max(distance, footprint=np.ones((3, 3)), labels=image)\n",
    "\n",
    "mask = np.zeros(distance.shape, dtype=bool)\n",
    "mask[tuple(coords.T)] = True\n",
    "\n",
    "plt.imshow(mask, cmap=plt.cm.gray)\n",
    "plt.title('Mask with centers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b38be-d3c6-4746-90a3-d90e247548c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(-distance, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers, _ = ndi.label(mask)\n",
    "labels = watershed(-distance, markers, mask=image)\n",
    "\n",
    "plt.imshow(labels, cmap=plt.cm.gray)\n",
    "plt.title('Separated objects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-marathon",
   "metadata": {},
   "source": [
    "### Процесс сегментации, реализуемый в библиотеке OpenCV\n",
    "Рассмотрим изображение монет, которые соприкасаются друг с другом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('coins.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(20, 8))\n",
    "plt.subplot(131),plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(132),plt.imshow(gray, cmap='gray')\n",
    "plt.title('Gray Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(133),plt.imshow(thresh, cmap='gray')\n",
    "plt.title('Binary Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-holly",
   "metadata": {},
   "source": [
    "Теперь нам нужно удалить шумы на изображении. Для этого мы можем использовать морфологическое размыкание. Чтобы удалить небольшие отверстия в объекте, мы можем использовать морфологическое замыкание. После выполнения данных операций мы точно знаем, что область рядом с центром объекта является передним планом, а область далеко от объекта - фоном. Единственная область, в которой мы не уверены, - это граничная область монет.\n",
    "\n",
    "Итак, нам нужно извлечь область, в которой мы уверены, что это монеты. Эрозия удаляет граничные пиксели. Это сработало бы, если бы предметы не касались друг друга. Но поскольку они касаются друг друга, еще одним хорошим вариантом было бы найти преобразование расстояния и применить правильный порог. Затем нужно найти область, в которой точно не располагаются монеты. Для этого применяем дилатацию. Дилатация увеличивает границу объекта до фона. Таким образом, мы можем убедиться, что любая область фона в результате действительно является фоном."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise removal\n",
    "kernel = np.ones((3,3), np.uint8)\n",
    "opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "# sure background area\n",
    "sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "\n",
    "# Finding sure foreground area\n",
    "dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "ret, sure_fg = cv2.threshold(dist_transform, 0.7*dist_transform.max(), 255, 0)\n",
    "\n",
    "# Finding unknown region\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "unknown = cv2.subtract(sure_bg, sure_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(20, 8))\n",
    "plt.subplot(131), plt.imshow(cv2.cvtColor(sure_fg, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Foreground - White'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(132), plt.imshow(sure_bg, cmap ='gray')\n",
    "plt.title('Background - Black'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(133), plt.imshow(unknown, cmap ='gray')\n",
    "plt.title('Unknown region - White'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-receiver",
   "metadata": {},
   "source": [
    "Теперь мы точно знаем, в каком регионе находятся монеты, где располагается фон. \n",
    "Далее мы создаем маркер (это массив того же размера, что и исходное изображение, но с типом данных int32) и маркируем области внутри него. Области, которые мы знаем наверняка (будь то передний план или фон), помечены любыми положительными целыми числами (разными целыми числами), а область, которую мы точно не знаем, просто оставляется равной нулю. Для этого мы используем cv.connectedComponents(). Он помечает фон изображения цифрой 0, затем другие объекты помечаются целыми числами, начиная с 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marker labelling\n",
    "ret, markers = cv2.connectedComponents(sure_fg)\n",
    "\n",
    "# Add one to all labels so that sure background is not 0, but 1\n",
    "markers = markers + 1\n",
    "\n",
    "# Now, mark the region of unknown with zero\n",
    "markers[unknown==255] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(14, 8))\n",
    "plt.subplot(121), plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(122), plt.imshow(markers, cmap=\"jet\")\n",
    "plt.title('Markers')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-courage",
   "metadata": {},
   "source": [
    "Применим алгоритм сегментации на основе водораздела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = cv2.watershed(img, markers)\n",
    "img[markers == -1] = [0, 0, 255]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(14, 8))\n",
    "plt.subplot(121), plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(122), plt.imshow(markers, cmap=\"jet\")\n",
    "plt.title('Markers')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-restriction",
   "metadata": {},
   "source": [
    "### Пример сегментации объектов с помощью skimage\n",
    "Далее представлен пример сегментации изображений с построением меток объектов. Применяются следующие шаги:\n",
    "- Установление пороговых значений бинаризации с помощью автоматического метода Оцу\n",
    "- Заполнение маленьких разрывов с помощью морфологического замыкания\n",
    "- Удаление артефактов соприкосновения границ объектов\n",
    "- Измерение областей изображения для фильтрации мелких посторонних объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data.coins()[50:-50, 50:-50]\n",
    "\n",
    "# apply threshold\n",
    "thresh = threshold_otsu(image)\n",
    "bw = closing(image > thresh, square(3))\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "plt.subplot(121), plt.imshow(image, cmap='gray')\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(122), plt.imshow(bw, cmap='gray')\n",
    "plt.title('Binary Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove artifacts connected to image border\n",
    "cleared = clear_border(bw)\n",
    "\n",
    "# label image regions\n",
    "label_image = label(cleared)\n",
    "\n",
    "# to make the background transparent, pass the value of `bg_label`,\n",
    "# and leave `bg_color` as `None` and `kind` as `overlay`\n",
    "image_label_overlay = label2rgb(label_image, image=image, bg_label=0)\n",
    "                                \n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "plt.subplot(121), plt.imshow(cleared, cmap = 'gray')\n",
    "plt.title('Cleared border Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(122), plt.imshow(label_image, cmap = 'gray')\n",
    "plt.title('Label Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.imshow(image_label_overlay)\n",
    "\n",
    "for i, region in enumerate(regionprops(label_image)):\n",
    "    \n",
    "    # take regions with large enough areas\n",
    "    if region.area >= 100:\n",
    "        print(f\"Object {i}: {region.bbox}\")\n",
    "        # draw rectangle around segmented coins\n",
    "        minr, minc, maxr, maxc = region.bbox\n",
    "        rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                  fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "ax.set_axis_off()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee0b337-53b4-4056-af71-ceff12b13bf2",
   "metadata": {},
   "source": [
    "## 3.5. Метод сегментации на основе извлечения признаков\n",
    "## 3.5.1. Извлечение морфометрических признаков объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0181c-b9c0-4514-a7f2-52f2e45ad665",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.zeros((600, 600))\n",
    "\n",
    "rr, cc = ellipse(300, 350, 100, 220)\n",
    "image[rr, cc] = 1\n",
    "\n",
    "image = rotate(image, angle=15, order=0)\n",
    "\n",
    "rr, cc = ellipse(100, 100, 60, 50)\n",
    "image[rr, cc] = 1\n",
    "\n",
    "label_img = label(image)\n",
    "regions = regionprops(label_img)\n",
    "\n",
    "plt.imshow(label_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d19df6-a53d-4dc8-963e-36f54c8fc21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image, cmap=plt.cm.gray)\n",
    "\n",
    "for props in regions:\n",
    "    y0, x0 = props.centroid\n",
    "    orientation = props.orientation\n",
    "    x1 = x0 + math.cos(orientation) * 0.5 * props.minor_axis_length\n",
    "    y1 = y0 - math.sin(orientation) * 0.5 * props.minor_axis_length\n",
    "    x2 = x0 - math.sin(orientation) * 0.5 * props.major_axis_length\n",
    "    y2 = y0 - math.cos(orientation) * 0.5 * props.major_axis_length\n",
    "\n",
    "    ax.plot((x0, x1), (y0, y1), '-r', linewidth=2.5)\n",
    "    ax.plot((x0, x2), (y0, y2), '-r', linewidth=2.5)\n",
    "    ax.plot(x0, y0, '.g', markersize=15)\n",
    "\n",
    "    minr, minc, maxr, maxc = props.bbox\n",
    "    bx = (minc, maxc, maxc, minc, minc)\n",
    "    by = (minr, minr, maxr, maxr, minr)\n",
    "    ax.plot(bx, by, '-b', linewidth=2.5)\n",
    "\n",
    "ax.axis((0, 600, 600, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4feac73-5509-4dc0-9a03-50b829a8b3f8",
   "metadata": {},
   "source": [
    "Мы используем skimage.measure.regionprops_table() для вычисления (выбранных) свойств для каждого региона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf17bd5-796c-475d-b605-afc34e699e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "props = regionprops_table(label_img, properties=('centroid',\n",
    "                                                 'orientation',\n",
    "                                                 'major_axis_length',\n",
    "                                                 'minor_axis_length'))\n",
    "pd.DataFrame(props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba9ab2-5375-4e1e-b975-bd775eebcdc9",
   "metadata": {},
   "source": [
    "## 3.5.2. Метод случайного леса для сегментации на основе локальных признаков random forests\n",
    "Сегментация на основе обучения вычисляется с использованием локальных функций на основе локальной интенсивности, краев и текстур в разных масштабах. Предоставляемая пользователем маска используется для идентификации различных областей. Пиксели маски используются для обучения классификатора случайного леса из scikit-learn. Затем неразмеченные пиксели маркируются на основе прогноза классификатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbfca75-41be-4432-86f3-14eb663ca43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_img = data.skin()\n",
    "\n",
    "plt.imshow(full_img)\n",
    "plt.show()\n",
    "\n",
    "img = full_img[:900, :900]\n",
    "\n",
    "# Build an array of labels for training the segmentation.\n",
    "# Here we use rectangles but visualization libraries such as plotly\n",
    "# can be used to draw a mask on the image.\n",
    "training_labels = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "training_labels[:130] = 1\n",
    "training_labels[:170, :400] = 1\n",
    "training_labels[600:900, 200:650] = 2\n",
    "training_labels[330:430, 210:320] = 3\n",
    "training_labels[260:340, 60:170] = 4\n",
    "training_labels[150:200, 720:860] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80bf92f-d602-4c8f-af7f-0a51b402e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_min = 1\n",
    "sigma_max = 16\n",
    "features_func = partial(feature.multiscale_basic_features,\n",
    "                        intensity=True, edges=False, texture=True,\n",
    "                        sigma_min=sigma_min, sigma_max=sigma_max,\n",
    "                        channel_axis=-1)\n",
    "features = features_func(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e646f6bc-3577-42c0-b0a4-1c2c8e00e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f97345-593f-48db-b9ea-4b356154d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(features[:,:,4], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8c77a-2d7e-4981-896d-59fc72d307be",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50, n_jobs=-1,\n",
    "                             max_depth=10, max_samples=0.05)\n",
    "clf = future.fit_segmenter(training_labels, features, clf)\n",
    "result = future.predict_segmenter(features, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b6dfb1-be6d-4474-935f-e11ad1f7a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(9, 10))\n",
    "ax[0].imshow(segmentation.mark_boundaries(img, result, mode='thick'))\n",
    "ax[0].contour(training_labels)\n",
    "ax[0].set_title('Image, mask and segmentation boundaries')\n",
    "ax[1].imshow(result)\n",
    "ax[1].set_title('Segmentation')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddbd8b7-7efc-4035-b109-800612b37ecd",
   "metadata": {},
   "source": [
    "## 3.5.3. Оценка важности признаков\n",
    "Ниже мы рассмотрим важность различных функций, рассчитанную с помощью scikit-learn. Характеристики интенсивности имеют гораздо большее значение, чем особенности текстуры. Может возникнуть соблазн использовать эту информацию для уменьшения количества признаков, предоставляемых классификатору, чтобы сократить время вычислений. Однако это может привести к переобучению и ухудшению результатов на границе между областями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e2e84-4890-4f47-aa30-2931ec99cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "l = len(clf.feature_importances_)\n",
    "\n",
    "feature_importance = (\n",
    "        clf.feature_importances_[:l//3],\n",
    "        clf.feature_importances_[l//3:2*l//3],\n",
    "        clf.feature_importances_[2*l//3:])\n",
    "\n",
    "sigmas = np.logspace(\n",
    "        np.log2(sigma_min), np.log2(sigma_max),\n",
    "        num=int(np.log2(sigma_max) - np.log2(sigma_min) + 1),\n",
    "        base=2, endpoint=True)\n",
    "\n",
    "for ch, color in zip(range(3), ['r', 'g', 'b']):\n",
    "    ax[0].plot(sigmas, feature_importance[ch][::3], 'o', color=color)\n",
    "    ax[0].set_title(\"Intensity features\")\n",
    "    ax[0].set_xlabel(\"$\\\\sigma$\")\n",
    "\n",
    "for ch, color in zip(range(3), ['r', 'g', 'b']):\n",
    "    ax[1].plot(sigmas, feature_importance[ch][1::3], 'o', color=color)\n",
    "    ax[1].plot(sigmas, feature_importance[ch][2::3], 's', color=color)\n",
    "    ax[1].set_title(\"Texture features\")\n",
    "    ax[1].set_xlabel(\"$\\\\sigma$\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cca662-55b4-49ec-a935-7163816dd7b7",
   "metadata": {},
   "source": [
    "## 3.5.4. Применение обученной модели к новому изображению\n",
    "Далее можно применить обученный классификатор к новым объектам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58127d86-6454-47cf-bd27-f80035fe9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_new = full_img[:700, 900:]\n",
    "\n",
    "plt.imshow(img_new)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "features_new = features_func(img_new)\n",
    "result_new = future.predict_segmenter(features_new, clf)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(6, 10))\n",
    "ax[0].imshow(segmentation.mark_boundaries(img_new, result_new, mode='thick'))\n",
    "ax[0].set_title('Image')\n",
    "ax[1].imshow(result_new)\n",
    "ax[1].set_title('Segmentation')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2610cac-9f34-4bac-a7e1-04e1717a2eaf",
   "metadata": {},
   "source": [
    "## 3.6. K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c21b0-bf85-4533-b21c-12a956dc24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('monarch.jpg')\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ef4135-138e-43f4-adfc-b000ee3f253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_vals = image.reshape((-1,3))\n",
    "pixel_vals = np.float32(pixel_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a25486-beaf-43c2-b0a3-0189590eddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.85)\n",
    "\n",
    "k = 3\n",
    "retval, labels, centers = cv2.kmeans(pixel_vals, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "centers = np.uint8(centers)\n",
    "segmented_data = centers[labels.flatten()]\n",
    "\n",
    "segmented_image = segmented_data.reshape((image.shape))\n",
    "\n",
    "plt.imshow(segmented_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2b1ac-968a-4db8-9883-4f654be11d2b",
   "metadata": {},
   "source": [
    "## 4. Практический пример отслеживания объекта\n",
    "Воспользуемся:\n",
    "- переходом в цветовую модель HSV\n",
    "- созданием маски для объекта с заданным цветом\n",
    "- морфологической обработкой полученной маски\n",
    "- нахождением контура объекта\n",
    "- вычислением наименьшей окружности, охватывающей объект\n",
    "- сохранением буфера последних точек локализации объекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зададим параметры отслеживаемого объекта в пространстве HSV\n",
    "colorLower = (24, 100, 100)\n",
    "colorUpper = (44, 255, 255)\n",
    "pts = deque(maxlen=64)\n",
    " \n",
    "# Создадим объект камеры\n",
    "camera = cv2.VideoCapture(0)\n",
    " \n",
    "# Работа с камерой в цикле\n",
    "while True:\n",
    "    # Прочитаем текущий кадр\n",
    "    (grabbed, frame) = camera.read()\n",
    "  \n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    frame = imutils.rotate(frame, angle=180)\n",
    "    \n",
    "    # Выполним переход в цветовую модель HSV\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    " \n",
    "    # Создадим маску для объекта с заданным цветом\n",
    "    mask = cv2.inRange(hsv, colorLower, colorUpper)\n",
    "    \n",
    "    # Морфологическая обработка маски\n",
    "    mask = cv2.erode(mask, None, iterations=2)\n",
    "    mask = cv2.dilate(mask, None, iterations=2)\n",
    "    \n",
    "    # Поиск контуров и центра объекта\n",
    "    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL,\n",
    "        cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "    center = None\n",
    " \n",
    "    # Если был найден контур\n",
    "    if len(cnts) > 0:\n",
    "        # найти наибольший контур по маске, затем использовать его\n",
    "        # для вычисления наименьшей окружности, охватывающей объект\n",
    "        c = max(cnts, key=cv2.contourArea)\n",
    "        ((x, y), radius) = cv2.minEnclosingCircle(c)\n",
    "        M = cv2.moments(c)\n",
    "        center = (int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"]))\n",
    " \n",
    "        if radius > 10:\n",
    "            # нарисовать окружность локализации объекта\n",
    "            cv2.circle(frame, (int(x), int(y)), int(radius),\n",
    "                (0, 255, 255), 2)\n",
    "            cv2.circle(frame, center, 5, (0, 0, 255), -1)\n",
    " \n",
    "    # обновить список точек\n",
    "    pts.appendleft(center)\n",
    "    \n",
    "    # Цикл по всем текущем (64) точкам локализации\n",
    "    for i in range(1, len(pts)):\n",
    "        if pts[i - 1] is None or pts[i] is None:\n",
    "            continue\n",
    " \n",
    "        # Нарисовать след объекта\n",
    "        thickness = int(np.sqrt(64 / float(i + 1)) * 2.5)\n",
    "        cv2.line(frame, pts[i - 1], pts[i], (0, 0, 255), thickness)\n",
    " \n",
    "    # Отобразить картинку на экране\n",
    "    frame = imutils.rotate(frame, angle=180)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "    # выйти из цикла обработки изображения\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Освободить ресурс камеры и закрыть всплывающее окно\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e670807b-31cc-4ded-8e8b-cfb67f1021aa",
   "metadata": {},
   "source": [
    "Пример классического метода сегментации:\n",
    "https://stackoverflow.com/questions/57813137/how-to-use-watershed-segmentation-in-opencv-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "cv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
